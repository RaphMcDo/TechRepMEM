# Methods

## Study Area

PUT FIGURE OF NAFO DIVISION LIKE IN PAPER, JUST 1 EXAMPLE TO SHOW WHAT KIND OF SPREAD WE ARE TALKING ABOUT.

The Atlantic halibut managements units in Canadian waters that we focus on for this report are the Scotian Shelf and Southern Grand Banks, which encompass the North Atlantic Fisheries Organization (NAFO) divisions 3NOPs4VWX5Zc. The other NAFO division 4RST encompasses the Gulf of St. Lawrence and will not be included in our analysis. The definition of this NAFO division is mostly based on tagging studies that showed Atlantic halibut moving throughout most of the Canadian North Atlantic [@DFO2021]. While the NAFO division extends outside of the Canadian Exclusive Economic Zone (EEZ) as seen in Figure WHATEVER, most of the Canadian fishing is done along the continental shelf by longline [@DFO2021]. 

Due to the nature of spatial modelling and our need to scale station-specific indices up to an overal index representing the whole area, the area modelled is bounded by the continental shelf as shown in Figure WHATEVER. As no survey ever extends outside of these edges and it would be statistically inapropriate to try to extend our model beyond the spatial cover of these observations, restricting our model to this area was considered appropriate.

## Survey designs

As mentioned previously, there are 2 surveys with different sampling designs that are tackled with this work: a survey that follows a fixed stations design that goes from 1998 to the present day, and a survey following a stratified random sampling design that started in 2017. 

### Fixed Stations

The original formulation of this survey followed a stratified design with fixed stations with the original number of stations set to 222, with 30 reserved for the 3NOPs subdivision [@DenHeyer2015]. The strata were defined based on the observed landings by trips between 1993 and 1997 between 3 categories: high catches (>250 kg), medium catches (50-249 kg) and low catches (<49 kg) with the number of planned stations proportionally allocated following a ratio of 5:7:10 for the low, medium, high strata respectively [@Zwaneburg2000,@Zwaneburg2003,@DenHeyer2015,@Smith2016a]. Station coverage has been inconsistent over time as not all stations have been covered every year with new stations being added in the mid-2000s [@DenHeyer2015], see Figure WHATEVER for locations of sampled stations over time. The number of fixed stations decreased to around 100 a year in 2017 and onwards due to the implementation of the new stratified random survey. See Table WHATEVER for the number of stations sampled every year.

Survey fishing protocol is 1000 hook sets left in the water for 10 hours and using Mustad circle hooks #14 or greater between 4 am and noon, but there have been variations in both number and size of hooks (size #16 hooks becoming more common later in the time series) [@DenHeyere2015]. The start or end of the longline is supposed to be within 3 nautical miles of the station. Vessel and captain participation has varied over time following many different factors, with over 40 individual captains and 60 different vessels identified over time [@Smith2016a].

PUT TABLE OF NUMBER OF STATIONS SAMPLED A YEAR WITH BOTH STRATIFIED AND FIXED AND TOTAL.

PUT FIGURE OF SAMPLED FIXED STATIONS EVERY YEAR

### Stratified Random Design

The sampling design for this survey follows a stratified random sampling design, where the strata are the 5 NAFO subdivisions (4X5YZ, 4W, 4V, 3P, 3NO) each with 3 depth strata (30-130 m, 131-250 m, 251-750 m) and includes 3Pn even if it is not explicitly part of the management area [@Cox2018,@Luo2022]. The depth bouds (30-750 m) were chose as they contain most of the survey sets, most of the Atlantic halibut habitat, and were based on exploratory analyses of catch rates by depth from the fixed stations [@Cox2018]. Each year and within these strata, around 150 survey stations are randomly assigned to strata with the number in a given stratum proportional to its size [@Cox2018,@Luo2022]. The fishing protocol are similar to the fixed stations protocol, with each set containing 1000 baited hooks with size #15 hooks set for between 6 and 12 hours [@Luo2022]. 

Unlike the fixed stations, the observers collecting the stratified random survey data must also record hook condition data on a subset of hooks for each set. Based on a pilot study that aimed to calculate the size required to be broadly representative of the whole set [@Doherty2017], 10 samples of 30 hooks across the line are chosen for this increased information collection for a total of 300 hooks per 1000 hooks set [@Luo2022]. Instead of simply counting the number of halibut and non-target species caught, the condition of each hook (baited, unbaited, broken, missing) is also recorded for this subsample. See Table WHATEVER for the number of stations sampled every year.

## Models

### MEM

The original aim of the MEM was to account for hook competition in longline fishing as proposed by @Rothschild1967 and reformulated by @Etienne2013. There are two different formulations of the MEM of interest here, which will respectively be called the Full MEM and the Reduced MEM. 

The reduced MEM allows for three possible outcomes for every hook following the summation $N_i = N_{B,i}+N_{T,i}+N_{NT,i}$, wherein the total number of hooks $N_i$ on a longline set $i$ is the sum of the number of hooks with non-target species $N_{NT,i}$, the number of hooks with target species $N_{T,i}$, and the number of hooks without any animals that are assumed to still be baited $N_{B,i}$. Assuming that the time to catch a target or non-target species follow independent exponential distributions with rates $\lambda_T$ and $\lambda_{NT}$, then the the vector $(N_{B,i},N_{T,i},N_{NT,i})$ follows a multinomial distribution [@Luo2022]:

\begin{equation}
(N_{B,i},N_{T,i},N_{NT,i}) ~ \mathcal{M}(N_i,\alpha_i)
\end{equation}
\begin{equation}
\alpha_i = (e^{-\lambda S_i},(1-e^{-\lambda S_i})\frac{\lambda_T}{\lambda},(1-e^{-\lambda S_i})\frac{\lambda_{NT}}{\lambda})
\end{equation}

where $S_i$ is the soak time of longline set $i$ and $\lambda = \lambda_T + \lambda_{NT}. 

This formulation did not account for the condition of hooks that return without catching animals, as these are not guaranteed to still be baited. @Etienne2013 therefore reformulated this MEM into the Full MEM, wherein hooks can also come back unbaited (which includes broken or missing hooks). These empty hooks are assumed to come from interactions with animals, but due to identifiability concerns another assumption has to be made as to whether they are caused by target or non-target species [@Etienne2013]. Since the gear is more likely to be targeted for target species and that non-target species are likely more abundant, it seemed more reasonable to assume that empty or broken hooks were caused by non-target species [@Etienne2013, @Luo2022]. This extra outcome is therefore added to the multinomial model, which nows follows the following distributions:

\begin{equation}
(N_{B,i},N_{T,i},N_{NT,i},N_{E,i}) ~ \mathcal{M}(N_i,\alpha_i)
\end{equation}
\begin{equation}
\alpha_i = (e^{-\lambda S_i},(1-e^{-\lambda S_i})\frac{\lambda_T}{\lambda},(1-e^{-\lambda S_i})\frac{\lambda_{NT}}{\lambda}(1-p_{NT}),(1-e^{-\lambda S_i})\frac{\lambda_{NT}p_{NT}}{\lambda})
\end{equation}

where $N_{E,i}$ is the number of empty hooks and $p_{NT}$ is the probability of escape of non-target animals. In this formulation, $p_{NT}$ shows up in the utilization of $N_{NT,i}$, as it impacts the catch rates of non-target species. As this component is present in the Reduced MEM and those non-target species would be similarly able to escape as in the Full MEM. For this work, we therefore modified the Reduced MEM so that the multinomial component $N_{NT,i}$ is obtained with $(1-e^{-\lambda S_i})\frac{\lambda_{NT}}{\lambda}(1-p_{NT})$. While it is unlikely that this version of the Reduced MEM can reliably estimate $p_{NT}$, it should be able to borrow the information from the Full MEM when they are fit together in the product likelihood method described later.

### MEMSpa

While the MEM model achieved its goal of incorporating hook competition into a model to obtain better estimates of catch rates, it did not account for spatial patterns in the survey data. @Luo2022 harnessed geostatistical approaches to modify both versions of the MEM through the addition of a hierarchical level to the exponential rates with the use of Gaussian Random Fields (GRF). 

The observation level of this new MEMSpa remained almost the same as described in the previous section, but the rates $\lambda_T$ and $\lambda_{NT}$ were modified to incorporate the location of a given longline set $i$:

\begin{equation}
\lambda_{T,i} = exp(\beta_T+\omega_{T,i})
\end{equation}
\begin{equation}
\lambda_{NT,i} = exp(\beta_{NT}+\omega_{NT,i})
\end{equation}

where $\lambda_{T,i}$ and $\lambda_{NT,i}$ are the exponential rates for target and non-target species at the location of longline set $i$, $\beta_T$ and $\beta_{NT}$ are intercept parameters, and $\omega_{T,i}$ and $\omega_{NT,i}$ are the values of the underlying GRF. 

These modifications allow the model to explicitly incorporate the spatial patterns present in the data to obtain station-specific catch rates. However, one of the aim is to be able to obtain a single overall estimated rate for the entire modelled area to be treated as an index of relative abundance. Due to the computational load of utilizing kriging to obtain this index, a Dirichlet method is utilized where the modelled area is divided into disjoint tiles based on the survey station locations, wherein each station is associated with a specific region and assumed to be representative of it [@Luo2022]. One can then obtain a spatially-weighted survey index for the whole area:

\begin{equation}
Overall Index = \frac{\sum_{i=1}^I A_i \hat{\lambda}_i}{\sum_{i=1}^I A_i}
\end{equation}

where $A_i$ is the area of the Dirichlet tile associated with station $i$, $\hat{\lambda}_i$ is the corresponding estimated catch rate at this station, and $I$ is the total number of stations.

As the data from the stratified random survey only has a subset of its hooks where the hook condition was recorded, a product likelihood approach was taken so as to incorporate all the available data inside a unified framework. This consists in separately fitting the Reduced MEMSpa to the data without hook condition and the Full MEMSpa to the data with hook condition, and multiplying their likelihoods together (see @Luo2022 for more details on MEMSpa).

### Spat-Temp MEM

The improvements brought to the table by the inclusion of spatial patterns in MEMSpa were very clear when fit to the data from the stratified, but it still left a lot of available information on the table since it could only be fit to the data from the stratified random survey, leaving over 20 years of unused data from the fixed stations survey. Furthermore, the spatial model can only be fit to a single year at a time and does not account for any temporal patterns in halibut or non-target species distributions and abundance. Incorporating these patterns alongside MEMSpa would therefore result in a fully spatio-temporal MEM.

As we aim to retain the inclusion of spatial patterns through the residual structure as done in MEMSpa, the temporal aspects would need to be incorporated (following the regression framework of MEMSpa) outside of this structure. There are many different approaches to the incorporation of temporal patterns in the mean for fisheries models which usually involve random effects, with common approaches including random intercepts (GIVE EXAMPLE), random slopes (GIVE EXAMPLES), random walks (GIVE EXAMPLES), or autoregressive models (GIVE EXAMPLES). These last two would also have to be seen as more structured versions of other approaches wherein the random walk or autoregressive structure would be on the random intercept. Furthermore, we also decided to incorporate the impact of different survey vessels through random vessel effects for our dataset. These approach would take the following forms:

\begin{equation}\label{eq:rand-int}
\lambda_{T,i,y} = exp(\eta_{T,y}+ + \nu_{T,j}+\omega_{T,i,y})
\end{equation}
\begin{equation}\label{eq:rand-slope}
\lambda_{T,i,y} = exp(\beta_T+\eta_{T,y}+ \nu_{T,j}+\omega_{T,i,y})
\end{equation}

where $\nu_{T,j}$ is the 0 mean normally distributed random effect of vessel $j$ with associated variance $\sigma_\nu^2$, $\eta_{T,y}$ are the random intercepts for target species in year $t$ or, in Equation \@ref(eq:rand-slope), the random slopes in year $t$ with global intercept $\beta_T$. For the random walk and autoregressive models, they would follow the same formulation as Equation \@ref(eq:rand-int) with the following added structure on $\eta_{T,y}$:

\begin{equation}\label{eq:rand-walk}
\eta_{T,y} = \eta_{T,y-1} + \epsilon_y, \ \ \ \epsilon_y ~ N(0,\sigma_\eta^2)
\end{equation}
\begin{equation}\label{eq:ar1}
\eta_{T,y} = c + \phi \eta_{T,y-1} + \epsilon_y, \ \ \ \epsilon_y ~ N(0,\sigma_\eta^2)
\end{equation}

where $\epsilon_y$ is a 0 mean normally distributed error term with variance $\sigma_\eta^2$. The autoregressive model is an AR(1) process with constant mean $c$ and autoregressive parameter $\phi$ bounded between -1 and 1 to ensure stationarity. Non-target rates would follow the same structure with non-target specific parameters.

These models are fit to the data from both surveys together. An important note for our analysis is that the fixed stations data does not contain any information on hook condition, meaning that only the Reduced MEM can be fit to it. Model validation will be done by comparing Root Mean Squared Errors (RMSE), calculating AIC and BIC values, and by performing 10-fold cross validation for the 4 different models. There are 2 types of errors to look at in this model, one being the GRF at the hierarchical level of the catch rates $\lambda$ and the other being the difference between the observations and the expected observations based on the probabilities obtained from the multinomial distribution. Either way, the RMSE is calculated as:

\begin{equation}
\frac{\sum_{i=1}^N e_i^2}{N}
\end{equation}

where $e$ is the error, either the GRF value $\omega_{i,y}$ for rate $i$ in year $y$ or the sum difference between observed number of target, non-target and hooks and the expected number of each respective values for a given longline set $i$.

AIC and BIC are both information criterion that is used to compare different model fits to data that try to balance between better model fit (through better likelihood) and number of parameters. They are respectively calculated as:

\begin{equation}\label{eq:aic}
AIC = 2K - 2 log(L)
\end{equation}
\begin{equation}\label{eq:bic}
BIC = K log(n) - 2 log(L)
\end{equation}

where $K$ is the number of parameters, $log(\cdot)$ is the natural log, $L$ is the maximized likelihood, and $n$ is the number of data points (CITE A SOURCE FOR BOTH OF THESE).

Lastly, the 10-fold cross-validation is done by randomly splitting your data in 10 different subsamples, iteratively fit your model to 90% of the dataset (training set) and using this model output to predict the leftover 10% (test set) until all the data has been both the test set and the training set. From these fits, one can calculate the prediction error for each models from all test sets and see which one predicts the test sets best.

## Fit to Data

Once the best model is chosen and to respect the requirements of the term of reference, the model will be applied to various subsets of the data to be able to compare and better understand the model output. These different data subsets are: only the stratified data (2017 to 2020), the fixed stations (1998 to 2020), both datasets in years where both are available (2017 to 2020) and both datasets for all years (1998 to 2020). 

## Persistence of spatial patterns

While this was not required by the contract, a quick analysis of the ability of the fixed stations data to accurately capture changes over time of Atlantic halibut and non-target species abundance was undertaken, mainly due to the fact that obtaining an unbiased estimate of population abundance when only using fixed stations is very difficult due to the absence of reliable design-based estimators [@Li2015,@Lee2018]. Furthermore, an underlying assumption behind this type of design is that the spatial distribution of the population of interest is consistent over time [@Li2015,@Lee2018]. In our case, this means that the distribution of Atlantic halibut and non-target species would have to be consistent with their spatial distribution between 1993 and 1997, as those were the years on which the fixed stations were based. This partially motivated the move towards the stratified random sampling design [@Smith2016], but the actual ability of our fixed stations to track population changes over time has not actually been tested. 

A relatively straightforward approach to test the fixed stations for their ability to appropriately track population abundance change over time is test the persistence of these distributions over time [@Lee2018]. As our data was counts and therefore violated the basic assumptions of the traditional ANOVA approach chosen by @Lee2018, we instead decided to take the more simple approach of doing pairwise comparisons between years of Atlantic halibut catch rates and non-target species catch rates, which can be calculated as follow [@Warren1994,@Li2015]: 

\begin{equation}
\overbar{\omega} = \frac{s^2_y/4}{s^2_s-s^2_y/4}
\end{equation}

where $\overbar{\omega}$ is the measurement of persistence degree wherein a smaller value indicates a greater degree of persistence, $s^2_y$ is the difference in catch rates of the same site between compared years, and $s^2_s$ is the difference in catch rates between different sites in the same year. These last 2 variables are calculated as:

\begin{equation}
s^2_s = \frac{\sum_{y=1}^2 \sum_{y=1}^{n_i} (x_{iy}-\overbar{x}_y)^2}{m_1+m_2-2}
\end{equation}
\begin{equation}
s^2_y = \sum_{i=1}^m (d_i - \overbar{d})^2 / (m-1)
\end{equation}

where $x_{iy}$ is the observed catch rate in site $i$ and year $y$, $\overbar{x}_y$ is the mean observed catch rate of year $y$, $m_1$ is the number of fixed stations in the first year included in the pairwise comparison while $m_2$ is the same for the second year, $d_i$ is the different in catch rate between two years in site $i$ and $\overbar{d}$ is the mean catch rate difference. An important note here is that, as the coverage of stations changed over time, there are often more stations included in the $s^2_s$ than $s^2_y$ as all stations in both years are included in $s^2_s$, but only the stations fished in both years can be included in $s^2_y$. This approach is done on the target species (halibut) catch rates and non-target catch rates separately for the fixed stations dataset between 2000 and 2020.
