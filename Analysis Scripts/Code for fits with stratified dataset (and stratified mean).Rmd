---
title: "All fits with stratified data"
author: "RaphaÃ«l McDonald"
date: "9/26/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r initial-data-vis}
library(dplyr)
library(sf)

library(TMB) 
# # Call TMB function value
# compile("multi_year_prod.cpp")#,"&> C:/Users/mcdonaldra/Documents/errors.txt")
# # Dynamically link the C++ code
# dyn.load(dynlib("multi_year_prod"))
# Call TMB function value
compile("spat_temp_both_datasets.cpp")#,"&> C:/Users/mcdonaldra/Documents/errors.txt")
# # Dynamically link the C++ code
dyn.load(dynlib("spat_temp_both_datasets"))


rand_data<-read.csv("RandomSurvey_anon.csv")
dim(rand_data)
unique(rand_data$YEAR)
#Confirmed that this is the same data, but it has 2021 now, which is fun

fixed_data<-read.csv("FixedSurvey_anon.csv")
dim(fixed_data)
unique(fixed_data$YEAR)
#contains a lot more data, all the way from 1998 to 2021, meaning I have twice the data for 2017-2021
colnames(fixed_data)

#The fixed data station does not appear to have the 300 hooks subset, that it is just the overall stuff from the 1000 hooks, meaning only the reduced multinomial model could be applied to it (once I've figured out the impact of a fixed survey design on the spatial model)

library(sp)
library(maptools)
library(rgeos)
library(PBSmapping)
library(ggplot2)

prj4s=CRS("+init=epsg:4326")
utm.prj4s=CRS("+init=epsg:32620")

sf_fixed_data<-st_as_sf(fixed_data, coords=c("LONGITUDE","LATITUDE"))
st_crs(sf_fixed_data)<-prj4s

survey_strats<-load("SurveyStrata.RData")
survey_stret<-as.PolySet(surveyStrataPolyLL)
sp_survey_strata_poly<-PolySet2SpatialPolygons(survey_stret)
sp_survey_strata_poly@proj4string<-prj4s
for(j in 1:15){
  for(i in 1:length(sp_survey_strata_poly@polygons[[j]]@Polygons)){
    if (sp_survey_strata_poly@polygons[[j]]@Polygons[[i]]@hole==F) {
    sp_survey_strata_poly@polygons[[j]]@Polygons[[i]]@hole<-T
  }
  if (sp_survey_strata_poly@polygons[[j]]@Polygons[[i]]@hole==T) {
    sp_survey_strata_poly@polygons[[j]]@Polygons[[i]]@hole<-F
  }
}
}

sf_survey_strata_poly<-st_as_sf(sp_survey_strata_poly)

#First, make a dataframe of number of tows a year before I remove sets that are bad
n_frame<-data.frame(Year=c(1998:2021),n=rep(NA,length(1998:2021)))

for (i in 1:nrow(n_frame)){
  n_frame$n[i]<-length(fixed_data$YEAR[fixed_data$YEAR==n_frame$Year[i]])
}

#Now check for sets to be removed because they make no sense

which(is.na(fixed_data$NUM_HOOK_HAUL))
#These 9 tows have NA for how many hooks were hauled, which makes no sense, and every single one of them, from 2 NAFO areas (4XN and 4WL), all different stations, but all from a single trip in 2009 (J09-0308)

fixed_data2<-fixed_data[-which(is.na(fixed_data$NUM_HOOK_HAUL)),]

#Check if the numbers don't make sense, i.e. there are more species caught than number of hooks
which(fixed_data2$NUM_HOOK_HAUL-fixed_data2$total_other_species-fixed_data2$total_target_species<0)
#Mostly different years or trips, so no trends
fixed_data3<-fixed_data2[-which(fixed_data2$NUM_HOOK_HAUL-fixed_data2$total_other_species-fixed_data2$total_target_species<0),]

#Put together the number of empty hooks (including baited, unbaited, broken or missing hooks)
fixed_data3$empty_hooks<-fixed_data3$NUM_HOOK_HAUL-fixed_data3$total_other_species-fixed_data3$total_target_species

#Some tows have what I feel is an unreasonably small amount of empty hooks, might be worth taking a peek into it as a verification measure at some point, but I have no reason to move forward from that

#There is a problem with 1999, so gotta look into it
fixed_99<-subset(fixed_data3,YEAR==1999)
#It is the year with the 6 hook thing, I am going to remove this one and see if it works afterward

#Getting closer but still not working
fixed_data3<-fixed_data3[-which(fixed_data3$empty_hooks==6),]
# fixed_data3<-fixed_data3[-which(fixed_data3$empty_hooks==195),]

sf_rem_fix_data<-st_as_sf(fixed_data3,coords=c("LONGITUDE","LATITUDE"))
st_crs(sf_rem_fix_data)<-prj4s
#Make into utm
sf_rem_fix_data<-st_transform(sf_rem_fix_data,utm.prj4s)
sf_rem_fix_data<-sf_rem_fix_data[-c(29:33),]

#Lets try removing 1998-1999, see what happens
sf_rem_fix_data<-subset(sf_rem_fix_data,YEAR > 1999)

#Now there are a few where soak time is NA, so have to remove those
sf_rem_fix_data<-sf_rem_fix_data[-which(is.na(sf_rem_fix_data$SOAKMINP3P1)),]

for (i in 1:nrow(n_frame)){
  n_frame$fix_n[i]<-length(sf_rem_fix_data$YEAR[sf_rem_fix_data$YEAR==n_frame$Year[i]])
}

# Logit function
logitp=function(p){log(p/(1-p))}
# Inverse logist function
logitpi=function(t){exp(t)/(1+exp(t))}

dat_list_for_comb<-list()
for (i in 2000:2021){
  dat_list_for_comb[[i-1999]]<-subset(sf_rem_fix_data,YEAR==i)
}

```

```{r strat-data}

# The halibut data for 2017
data2017 = rand_data[which(rand_data$YEAR == 2017), ]
dim(data2017)
# Drop the stations (523 and 525) with hook = 30
surv_not30 = which(data2017$total_sampled == 30)
data2017=data2017[-c(surv_not30),]
which(data2017$hooks_sampled == 30)
dim(data2017)
# Drop Stations with total number of non-target fish 
# with greater than total number of hooks
g_index=which(data2017$NUM_HOOK_HAUL-data2017$total_other_species-data2017$total_target_species<0)
data2017=data2017[-g_index, ]
which(data2017$NUM_HOOK_HAUL-data2017$total_other_species-data2017$total_target_species<0)
dim(data2017)
# Drop Stations with 
# (total number of non-target species - number of non-target species from sampled hooks < 0)
g_index2=which(data2017$total_other_species-data2017$other_species<0)
data2017=data2017[-c(g_index2),]
which(data2017$total_other_species-data2017$other_species<0)
dim(data2017)
# Drop Stations with 
# (total number of target species - number of target species from sampled hooks < 0)
g_index3=which(data2017$total_target_species-data2017$target_species<0)
data2017=data2017[-c(g_index3),]
which(data2017$total_target_species-data2017$target_species<0)
dim(data2017)
# Drop Stations
g_index4=which(data2017$NUM_HOOK_HAUL-data2017$total_sampled-data2017$total_target_species-
                 data2017$target_species-data2017$total_other_species-data2017$other_species<0)
data2017=data2017[-c(g_index4),]
which(data2017$NUM_HOOK_HAUL-data2017$total_sampled-data2017$total_target_species-
        data2017$target_species-data2017$total_other_species-data2017$other_species<0)
dim(data2017)

# For 300 hooks
# Empty unbaited = Empty unbaited + missing hooks + broken hooks 
data2017$empty_unb=data2017$empty_unbaited+data2017$missing_hook+data2017$broken_hook
# Check
data2017$total_sampled-data2017$empty_baited-data2017$target_species-data2017$other_species-data2017$empty_unb
st_17_300=data2017$SOAKMINP3P1

# For 700 hooks
data2017$target_species_700=data2017$total_target_species-data2017$target_species
data2017$other_species_700=data2017$total_other_species-data2017$other_species
data2017$nbe_700=data2017$NUM_HOOK_HAUL-data2017$total_sampled-data2017$target_species_700-data2017$other_species_700

data2017<-st_as_sf(data2017,coords=c("LONGITUDE","LATITUDE"),crs=prj4s) %>% st_transform(utm.prj4s)

#2018
data2018 = rand_data[which(rand_data$YEAR == 2018), ]
dim(data2018)

# Drop Stations with total number of non-target fish 
# with greater than total number of hooks
g_index_18=which(data2018$NUM_HOOK_HAUL-data2018$total_other_species-data2018$total_target_species<0)
g_index_18
# Drop Stations with 
# (total number of non-target species - number of non-target species from sampled hooks < 0)
g_index2_18=which(data2018$total_other_species-data2018$other_species<0)
data2018=data2018[-c(g_index2_18),]
which(data2018$total_other_species-data2018$other_species<0)
dim(data2018)
# Drop Stations with 
# (total number of target species - number of target species from sampled hooks < 0)
g_index3_18=which(data2018$total_target_species-data2018$target_species<0)
g_index3_18
# Drop Stations
g_index4_18=which(data2018$NUM_HOOK_HAUL-data2018$total_sampled-data2018$total_target_species
               -data2018$target_species-data2018$total_other_species-data2018$other_species<0)
data2018=data2018[-c(g_index4_18),]
which(data2018$NUM_HOOK_HAUL-data2018$total_sampled-data2018$total_target_species
               -data2018$target_species-data2018$total_other_species
               -data2018$other_species<0)
dim(data2018)

# For 300 hooks
# Empty unbaited = Empty unbaited + missing hooks + broken hooks 
data2018$empty_unb=data2018$empty_unbaited+data2018$missing_hook+data2018$broken_hook
# Check
data2018$total_sampled-data2018$empty_baited-data2018$target_species-data2018$other_species-data2018$empty_unb
st_18_300=data2018$SOAKMINP3P1

# For 700 hooks
data2018$target_species_700=data2018$total_target_species-data2018$target_species
data2018$other_species_700=data2018$total_other_species-data2018$other_species
data2018$nbe_700=data2018$NUM_HOOK_HAUL-data2018$total_sampled-data2018$target_species_700-data2018$other_species_700

data2018<-st_as_sf(data2018,coords=c("LONGITUDE","LATITUDE"),crs=prj4s) %>% st_transform(utm.prj4s)

data2019 = rand_data[which(rand_data$YEAR == 2019), ]
dim(data2019)
# Five stations are with non-integer label
stat_error = sapply(data2019$STATION, function(i) i == as.integer(i))
stat_error_index = which(stat_error == "FALSE")
stat_error_index
data2019[stat_error_index,]
data2019[stat_error_index,]$STATION
data2019[stat_error_index,]$STATION = as.integer(data2019[stat_error_index,]$STATION)

# Drop Stations with total number of non-target fish 
# with greater than total number of hooks
g_index_19=which(data2019$NUM_HOOK_HAUL-data2019$total_other_species-data2019$total_target_species<0)
data2019=data2019[-c(g_index_19),]
which(data2019$NUM_HOOK_HAUL-data2019$total_other_species-data2019$total_target_species<0)
dim(data2019)
# Drop Stations with 
# (total number of non-target species - number of non-target species from sampled hooks < 0)
g_index2_19=which(data2019$total_other_species-data2019$other_species<0)
data2019=data2019[-c(g_index2_19),]
which(data2019$total_other_species-data2019$other_species<0)
dim(data2019)
# Drop Stations with 
# (total number of target species - number of target species from sampled hooks < 0)
g_index3_19=which(data2019$total_target_species-data2019$target_species<0)
g_index3_19
# Drop Stations
g_index4_19=which(data2019$NUM_HOOK_HAUL-data2019$total_sampled-data2019$total_target_species-
                    data2019$target_species-data2019$total_other_species-data2019$other_species<0)
g_index4_19

# For 300 hooks
# Empty unbaited = Empty unbaited + missing hooks + broken hooks 
data2019$empty_unb=data2019$empty_unbaited+data2019$missing_hook+data2019$broken_hook
# Check
data2019$total_sampled-data2019$empty_baited-data2019$target_species-data2019$other_species-data2019$empty_unb
st_19_300=data2019$SOAKMINP3P1

# For 700 hooks
data2019$target_species_700=data2019$total_target_species-data2019$target_species
data2019$other_species_700=data2019$total_other_species-data2019$other_species
data2019$nbe_700=data2019$NUM_HOOK_HAUL-data2019$total_sampled-data2019$target_species_700-data2019$other_species_700

data2019<-st_as_sf(data2019,coords=c("LONGITUDE","LATITUDE"),crs=prj4s) %>% st_transform(utm.prj4s)

# The halibut data for 2020
data2020 = rand_data[which(rand_data$YEAR == 2020), ]
dim(data2020)

#None of the tows have less than 30, the lowest is 180
unique(data2020$total_sampled)
#Compared to 2017, alll above 270 except for the 30
#Compared to 2018, there is atleast 1 180 that is used, but I will absolutely have to check that there are not stations where there are more fish than hooks
#Same for 2019

#Check and remove those bad stations
# Drop Stations with total number of non-target fish 
# with greater than total number of hooks
#There are none, easy then
g_index=which(data2020$NUM_HOOK_HAUL-data2020$total_other_species-data2020$total_target_species<0)
# data2020=data2020[-g_index, ]
which(data2020$NUM_HOOK_HAUL-data2020$total_other_species-data2020$total_target_species<0)
dim(data2020)

# Drop Stations with 
# (total number of non-target species - number of non-target species from sampled hooks < 0)
#There are 3
g_index2=which(data2020$total_other_species-data2020$other_species<0)
data2020=data2020[-c(g_index2),]
which(data2020$total_other_species-data2020$other_species<0)
dim(data2020)

# Drop Stations with 
# (total number of target species - number of target species from sampled hooks < 0)
#none!
g_index3=which(data2020$total_target_species-data2020$target_species<0)
# data2020=data2020[-c(g_index3),]
which(data2020$total_target_species-data2020$target_species<0)
dim(data2020)

# Drop Stations
#none for this too!
g_index4=which(data2020$NUM_HOOK_HAUL-data2020$total_sampled-data2020$total_target_species-
                 data2020$target_species-data2020$total_other_species-data2020$other_species<0)
# data2020=data2020[-c(g_index4),]
which(data2020$NUM_HOOK_HAUL-data2020$total_sampled-data2020$total_target_species-
        data2020$target_species-data2020$total_other_species-data2020$other_species<0)
dim(data2020)

# For 300 hooks
# Empty unbaited = Empty unbaited + missing hooks + broken hooks 
data2020$empty_unb=data2020$empty_unbaited+data2020$missing_hook+data2020$broken_hook
# Check
data2020$total_sampled-data2020$empty_baited-data2020$target_species-data2020$other_species-data2020$empty_unb
st_20_300=data2020$SOAKMINP3P1

# For 700 hooks
data2020$target_species_700=data2020$total_target_species-data2020$target_species
data2020$other_species_700=data2020$total_other_species-data2020$other_species
data2020$nbe_700=data2020$NUM_HOOK_HAUL-data2020$total_sampled-data2020$target_species_700-data2020$other_species_700

data2020<-st_as_sf(data2020,coords=c("LONGITUDE","LATITUDE"),crs=prj4s) %>% st_transform(utm.prj4s)

data2021 = rand_data[which(rand_data$YEAR == 2021), ]
dim(data2021)

#Some NAs, remove
data2021<-data2021[!is.na(data2021$total_sampled),]

#None of the tows have less than 30, the lowest is 180
unique(data2021$total_sampled)
#Compared to 2017, alll above 270 except for the 30
#Compared to 2018, there is atleast 1 180 that is used, but I will absolutely have to check that there are not stations where there are more fish than hooks
#Same for 2019

#Check and remove those bad stations
# Drop Stations with total number of non-target fish 
# with greater than total number of hooks
#There are none, easy then
g_index=which(data2021$NUM_HOOK_HAUL-data2021$total_other_species-data2021$total_target_species<0)
# data2021=data2021[-g_index, ]
which(data2021$NUM_HOOK_HAUL-data2021$total_other_species-data2021$total_target_species<0)
dim(data2021)

# Drop Stations with 
# (total number of non-target species - number of non-target species from sampled hooks < 0)

g_index2=which(data2021$total_other_species-data2021$other_species<0)
data2021=data2021[-c(g_index2),]
which(data2021$total_other_species-data2021$other_species<0)
dim(data2021)

# Drop Stations with 
# (total number of target species - number of target species from sampled hooks < 0)
#none!
g_index3=which(data2021$total_target_species-data2021$target_species<0)
# data2021=data2021[-c(g_index3),]
which(data2021$total_target_species-data2021$target_species<0)
dim(data2021)

# Drop Stations
#none for this too!
g_index4=which(data2021$NUM_HOOK_HAUL-data2021$total_sampled-data2021$total_target_species-
                 data2021$target_species-data2021$total_other_species-data2021$other_species<0)
# data2021=data2021[-c(g_index4),]
which(data2021$NUM_HOOK_HAUL-data2021$total_sampled-data2021$total_target_species-
        data2021$target_species-data2021$total_other_species-data2021$other_species<0)
dim(data2021)

# For 300 hooks
# Empty unbaited = Empty unbaited + missing hooks + broken hooks 
data2021$empty_unb=data2021$empty_unbaited+data2021$missing_hook+data2021$broken_hook
# Check
data2021$total_sampled-data2021$empty_baited-data2021$target_species-data2021$other_species-data2021$empty_unb
st_21_300=data2021$SOAKMINP3P1

# For 700 hooks
data2021$target_species_700=data2021$total_target_species-data2021$target_species
data2021$other_species_700=data2021$total_other_species-data2021$other_species
data2021$nbe_700=data2021$NUM_HOOK_HAUL-data2021$total_sampled-data2021$target_species_700-data2021$other_species_700

data2021<-st_as_sf(data2021,coords=c("LONGITUDE","LATITUDE"),crs=prj4s) %>% st_transform(utm.prj4s)

strat_dat_comb<-list()
for (i in 2000:2021){
  strat_dat_comb[[i-1999]]<-NULL
  if(i==2017) strat_dat_comb[[i-1999]]<-data2017
  if(i==2018) strat_dat_comb[[i-1999]]<-data2018
  if(i==2019) strat_dat_comb[[i-1999]]<-data2019
  if(i==2020) strat_dat_comb[[i-1999]]<-data2020
  if(i==2021) strat_dat_comb[[i-1999]]<-data2021
}

n_strat<-sum(c(nrow(data2017),nrow(data2018),nrow(data2019),nrow(data2020),nrow(data2021)))
n_strats_sep<-c(nrow(data2017),nrow(data2018),nrow(data2019),nrow(data2020),nrow(data2021))
n_fixed<-nrow(sf_rem_fix_data)

#Setting up H_i and H_j
H_j<-rep(NA,n_strat+n_fixed)
H_i<-rep(NA,n_strat)
vess_id<-rep(NA,n_strat+n_fixed)

n_t<-length(2000:2021)
n_k<-3
n_k2<-4

main_n2<-rep(NA,n_t)
for (i in 1:n_t){
  if (i < 18) main_n2[i]<-0
  else main_n2[i]<-n_strats_sep[i-17]
}

main_n<-rep(NA,n_t)
for (i in 1:n_t){
  if (i<18) main_n[i]<-n_frame$fix_n[i+2]
  else main_n[i]<-n_frame$fix_n[i+2]+main_n2[i]
}

sub_n_frame<-n_frame[-c(1,2),]

Yearly_indices<-data.frame(year=c(2000:2021),
                           ind_1=c(1,217,407,606,794,1009,1173,1336,1577,1858,2063,2278,2495,2712,2945,3177,3409,3636,3875,4125,4346,4593),
                           ind_2=c(216,406,605,793,1008,1172,1335,1576,1857,2062,2277,2494,2711,2944,3176,3408,3635,3874,4124,4345,4592,4821))
ind1<-Yearly_indices$ind_1
ind2<-Yearly_indices$ind_2

soaky<-rep(NA,sum(main_n))

locations<-matrix(nrow=sum(main_n),ncol=2)

big_A<-matrix(nrow=sum(main_n),ncol=n_k)
for (i in 2000:2021){
  if (i <2017){
    big_A[c((ind1[i-1999]):ind2[i-1999]),]<-as.matrix(data.frame(dat_list_for_comb[[i-1999]]$total_target_species,dat_list_for_comb[[i-1999]]$total_other_species,dat_list_for_comb[[i-1999]]$empty_hooks))
    H_j[c((ind1[i-1999]):ind2[i-1999])]<-rep(0,length(c((main_n[i-1999]):main_n[i-1999])))
    vess_id[c((ind1[i-1999]):ind2[i-1999])]<-dat_list_for_comb[[i-1999]]$Vess_fact
    soaky[c((ind1[i-1999]):ind2[i-1999])]<-dat_list_for_comb[[i-1999]]$SOAKMINP3P1
    locations[c((ind1[i-1999]):ind2[i-1999]),]<-st_coordinates(dat_list_for_comb[[i-1999]])
  } else if (i>2016){
    big_A[c(ind1[i-1999]:(ind1[i-1999]+sub_n_frame$fix_n[i-1999]-1)),]<-as.matrix(data.frame(dat_list_for_comb[[i-1999]]$total_target_species,dat_list_for_comb[[i-1999]]$total_other_species,dat_list_for_comb[[i-1999]]$empty_hooks))
    H_j[c((ind1[i-1999]):(ind1[i-1999]+sub_n_frame$fix_n[i-1999]-1))]<-rep(0,length(c((ind1[i-1999]):(ind1[i-1999]+sub_n_frame$fix_n[i-1999]-1))))
    vess_id[c((ind1[i-1999]):(ind1[i-1999]+sub_n_frame$fix_n[i-1999]-1))]<-dat_list_for_comb[[i-1999]]$Vess_fact
    soaky[c((ind1[i-1999]):(ind1[i-1999]+sub_n_frame$fix_n[i-1999]-1))]<-dat_list_for_comb[[i-1999]]$SOAKMINP3P1
    locations[c((ind1[i-1999]):(ind1[i-1999]+sub_n_frame$fix_n[i-1999]-1)),]<-st_coordinates(dat_list_for_comb[[i-1999]])
    big_A[c((ind1[i-1999]+sub_n_frame$fix_n[i-1999]):ind2[i-1999]),]<-as.matrix(data.frame(strat_dat_comb[[i-1999]]$target_species_700,strat_dat_comb[[i-1999]]$other_species_700,strat_dat_comb[[i-1999]]$nbe_700))
  H_j[c((ind1[i-1999]+sub_n_frame$fix_n[i-1999]):ind2[i-1999])]<-rep(1,length(c((ind1[i-1999]+sub_n_frame$fix_n[i-1999]):ind2[i-1999])))
  vess_id[c((ind1[i-1999]+sub_n_frame$fix_n[i-1999]):ind2[i-1999])]<-strat_dat_comb[[i-1999]]$Vess_fact
  soaky[c((ind1[i-1999]+sub_n_frame$fix_n[i-1999]):ind2[i-1999])]<-strat_dat_comb[[i-1999]]$SOAKMINP3P1
  locations[c((ind1[i-1999]+sub_n_frame$fix_n[i-1999]):ind2[i-1999]),]<-st_coordinates(strat_dat_comb[[i-1999]])
  }
}
H_i<-which(H_j==1)-1
vess_id<-as.integer(as.factor(vess_id))
vess_id<-vess_id-1

spool<-sqrt(((length(locations[,1])-1)*var(locations[,1])+(length(locations[,2])-1)*var(locations[,2]))/(length(locations[,1])+length(locations[,2])))
locations[,1]<-(locations[,1]-median(locations[,1]))/spool
locations[,2]<-(locations[,2]-median(locations[,2]))/spool


big_H<-as.matrix(data.frame(c(data2017$empty_baited,data2018$empty_baited,data2019$empty_baited,data2020$empty_baited,data2021$empty_baited),c(data2017$target_species,data2018$target_species,data2019$target_species,data2020$target_species,data2021$target_species),c(data2017$other_species,data2018$other_species,data2019$other_species,data2020$other_species,data2021$other_species),c(data2017$empty_unb,data2018$empty_unb,data2019$empty_unb,data2020$empty_unb,data2021$empty_unb)))

#Look up at which vessels are only present in single years
berple<-rbind(data2017,data2018,data2019,data2020,data2021)
n_years_vess<-rep(NA,87)
for (i in 1:87){
  garth<-subset(sf_rem_fix_data,Vess_fact == i)
  garth2<-subset(berple,Vess_fact==i)
  n_years_vess[i]<-length(unique(garth$YEAR))+length(unique(garth2$YEAR))
}

#Look at how many stations in a given year for each year
n_vess_year<-rep(NA,22)
for (i in 1:22){
  garth3<-subset(sf_rem_fix_data,YEAR==i+1999)
  garth4<-subset(berple,YEAR==i+1999)
  n_vess_year[i]<-length(unique(garth3$Vess_fact))+length(unique(garth4$Vess_fact))
}

n_tows_years<-matrix(nrow=87,ncol=22)
for (i in 1:87){
  for (j in 1:22){
    n_tows_years[i,j]<-nrow(subset(sf_rem_fix_data,YEAR==j+1999 & Vess_fact==i))+nrow(subset(berple,YEAR==j+1999 & Vess_fact==i))
  }
}

#Setup for TMB
data<-list(A=big_A,H=big_H,H_i=H_i,H_j=H_j,vess_id=vess_id,s=soaky,locations=locations,n_t=n_t,n=main_n,n_k=3,n2=main_n2,n_k2=4)

par_list<-list()
    
# Initial values for lambda.t, lambda.nt and pnt
# Use the estimated values as the starting points
par_list$betat = -12
par_list$betant = -8
# par_list$theta = logitp(0.8795768)
par_list$theta = 0
#Random effect of time
par_list$rand_t = rep(-12,data$n_t)
par_list$rand_nt = rep(-8,data$n_t)
# Random field
par_list$omegat =  rep(0,nrow(big_A))
par_list$omegant =  rep(0,nrow(big_A))
#Vessel effect
par_list$vess_eff_t = rep(0,length(unique(vess_id)))
par_list$vess_eff_nt = rep(0,length(unique(vess_id)))
# par_list$vess_eff_t = rep(0,length(unique(ident_fact)))
# par_list$vess_eff_nt = rep(0,length(unique(ident_fact)))
# Smoothness parameter 
par_list$lognut = 0
par_list$lognunt = 0
# Range parameter 
# par_list$logPhit = -3
# par_list$logPhint = -1
par_list$logPhit = -1
par_list$logPhint = -1
# Variance
par_list$logSigmat = 0
par_list$logSigmant = 0
#Vess effect sigmas
par_list$log_sigma_vess_t = 0
par_list$log_sigma_vess_nt = 0
#Rand intercept sigmas
par_list$log_sigma_rand_t = 0
par_list$log_sigma_rand_nt = 0
#Rand intercept means
par_list$mean_rand_t = -12
par_list$mean_rand_nt = -8
par_list$mean_ar_t = -12
par_list$mean_ar_nt = -8

par_list$logit_phi_ar_t= 0
par_list$logit_phi_ar_nt = 0

map<-list(lognut=factor(NA),lognunt=factor(NA))

random<-c("omegat","omegant","rand_t","rand_nt","vess_eff_t","vess_eff_nt")
library(optimr)

non_r<-names(par_list)[-which(names(par_list) %in% random)]

```

```{r fits-full-length}
#Fitting full dataset to all 4 fits of interest

compile("spat_temp_both_datasets.cpp")
dyn.load(dynlib("spat_temp_both_datasets"))

obj_walk<-MakeADFun(data,par_list,random=random,DLL="spat_temp_both_datasets",map=map,silent=F)
Opt_walk<-optimx::optimr(obj_walk$par,obj_walk$fn,obj_walk$gr,control=list(maxit=100000),method="nlminb")
if (Opt_walk$message=="iteration limit reached without convergence (10)") {
      obj_walk$par<-obj_walk$env$last.par[which(names(obj_walk$env$last.par) %in% non_r)]
      Opt_walk <- try(optimx::optimr(obj_walk$par,obj_walk$fn,obj_walk$gr,control=control,method="nlminb"),T)
    }
rep_walk<-sdreport(obj_walk)
Report_walk<-obj_walk$report()

save(obj_walk,Opt_walk,rep_walk,Report_walk,file="rand_walk_output.RData")

dyn.unload(dynlib("spat_temp_both_datasets"))
compile("spat_temp_mean.cpp")
dyn.load(dynlib("spat_temp_mean"))

obj_mean<-MakeADFun(data,par_list,random=random,DLL="spat_temp_mean",map=map,silent=F)
Opt_mean<-optimx::optimr(obj_mean$par,obj_mean$fn,obj_mean$gr,control=list(maxit=100000),method="nlminb")
if (Opt_mean$message=="iteration limit reached without convergence (10)") {
      obj_mean$par<-obj_mean$env$last.par[which(names(obj_mean$env$last.par) %in% non_r)]
      Opt_mean <- try(optimx::optimr(obj_mean$par,obj_mean$fn,obj_mean$gr,control=control,method="nlminb"),T)
    }
rep_mean<-sdreport(obj_mean)
Report_mean<-obj_mean$report()

save(obj_mean,Opt_mean,rep_mean,Report_mean,file="rand_mean_output.RData")

dyn.unload(dynlib("spat_temp_mean"))
compile("spat_temp_slope.cpp")
dyn.load(dynlib("spat_temp_slope"))

par_list$rand_t = rep(0,data$n_t)
par_list$rand_nt = rep(0,data$n_t)

obj_slope<-MakeADFun(data,par_list,random=random,DLL="spat_temp_slope",map=map,silent=F)
Opt_slope<-optimx::optimr(obj_slope$par,obj_slope$fn,obj_slope$gr,control=list(maxit=100000),method="nlminb")
if (Opt_slope$message=="iteration limit reached without convergence (10)") {
      obj_slope$par<-obj_slope$env$last.par[which(names(obj_slope$env$last.par) %in% non_r)]
      Opt_slope <- try(optimx::optimr(obj_slope$par,obj_slope$fn,obj_slope$gr,control=control,method="nlminb"),T)
    }
rep_slope<-sdreport(obj_slope)
Report_slope<-obj_slope$report()

save(obj_slope,Opt_slope,rep_slope,Report_slope,file="rand_slope_output.RData")

dyn.unload(dynlib("spat_temp_slope"))
compile("spat_temp_AR1.cpp")
dyn.load(dynlib("spat_temp_AR1"))

par_list$rand_t = rep(-12,data$n_t)
par_list$rand_nt = rep(-8,data$n_t)

obj_AR1<-MakeADFun(data,par_list,random=random,DLL="spat_temp_AR1",map=map,silent=F)
Opt_AR1<-optimx::optimr(obj_AR1$par,obj_AR1$fn,obj_AR1$gr,control=list(maxit=100000),method="nlminb")
if (Opt_AR1$message=="iteration limit reached without convergence (10)") {
      obj_AR1$par<-obj_AR1$env$last.par[which(names(obj_AR1$env$last.par) %in% non_r)]
      Opt_AR1 <- try(optimx::optimr(obj_AR1$par,obj_AR1$fn,obj_AR1$gr,control=control,method="nlminb"),T)
    }
rep_AR1<-sdreport(obj_AR1)
Report_AR1<-obj_AR1$report()

save(obj_AR1,Opt_AR1,rep_AR1,Report_AR1,file="rand_AR1_output.RData")

dyn.unload(dynlib("spat_temp_AR1"))

```



```{r full-datasets-2017}
n_strat<-sum(c(nrow(data2017),nrow(data2018),nrow(data2019),nrow(data2020),nrow(data2021)))
n_strats_sep<-c(nrow(data2017),nrow(data2018),nrow(data2019),nrow(data2020),nrow(data2021))
n_fixed<-nrow(subset(sf_rem_fix_data,YEAR %in% c(2017:2021)))

#Setting up H_i and H_j
H_j<-rep(NA,n_strat+n_fixed)
H_i<-rep(NA,n_strat)
vess_id<-rep(NA,n_strat+n_fixed)

n_t<-length(2017:2021)
n_k<-3
n_k2<-4

main_n2<-rep(NA,n_t)
for (i in 1:n_t){
  main_n2[i]<-n_strats_sep[i]
}

main_n<-rep(NA,n_t)
for (i in 1:n_t){
  main_n[i]<-n_frame$fix_n[i+19]+main_n2[i]
}

sub_n_frame<-n_frame[c(20:24),]

#NEED TO MODIFY STARTING HERE
Yearly_indices<-data.frame(year=c(2017:2021),
                           ind_1=c(1,240,490,711,958),
                           ind_2=c(239,489,710,957,1186))
ind1<-Yearly_indices$ind_1
ind2<-Yearly_indices$ind_2

soaky<-rep(NA,sum(main_n))

locations<-matrix(nrow=sum(main_n),ncol=2)

sub_dat_list_for_comb<-list(dat_list_for_comb[[18]],dat_list_for_comb[[19]],dat_list_for_comb[[20]],dat_list_for_comb[[21]],dat_list_for_comb[[22]])

sub_strat_dat_comb<-list(strat_dat_comb[[18]],strat_dat_comb[[19]],strat_dat_comb[[20]],strat_dat_comb[[21]],strat_dat_comb[[22]])

big_A<-matrix(nrow=sum(main_n),ncol=n_k)
for (i in 2017:2021){
    big_A[c(ind1[i-2016]:(ind1[i-2016]+sub_n_frame$fix_n[i-2016]-1)),]<-as.matrix(data.frame(sub_dat_list_for_comb[[i-2016]]$total_target_species,sub_dat_list_for_comb[[i-2016]]$total_other_species,sub_dat_list_for_comb[[i-2016]]$empty_hooks))
    H_j[c((ind1[i-2016]):(ind1[i-2016]+sub_n_frame$fix_n[i-2016]-1))]<-rep(0,length(c((ind1[i-2016]):(ind1[i-2016]+sub_n_frame$fix_n[i-2016]-1))))
    soaky[c((ind1[i-2016]):(ind1[i-2016]+sub_n_frame$fix_n[i-2016]-1))]<-sub_dat_list_for_comb[[i-2016]]$SOAKMINP3P1
    locations[c((ind1[i-2016]):(ind1[i-2016]+sub_n_frame$fix_n[i-2016]-1)),]<-st_coordinates(sub_dat_list_for_comb[[i-2016]])
    vess_id[c((ind1[i-2016]):(ind1[i-2016]+sub_n_frame$fix_n[i-2016]-1))]<-sub_dat_list_for_comb[[i-2016]]$Vess_fact
    big_A[c((ind1[i-2016]+sub_n_frame$fix_n[i-2016]):ind2[i-2016]),]<-as.matrix(data.frame(sub_strat_dat_comb[[i-2016]]$target_species_700,sub_strat_dat_comb[[i-2016]]$other_species_700,sub_strat_dat_comb[[i-2016]]$nbe_700))
  H_j[c((ind1[i-2016]+sub_n_frame$fix_n[i-2016]):ind2[i-2016])]<-rep(1,length(c((ind1[i-2016]+sub_n_frame$fix_n[i-2016]):ind2[i-2016])))
  soaky[c((ind1[i-2016]+sub_n_frame$fix_n[i-2016]):ind2[i-2016])]<-sub_strat_dat_comb[[i-2016]]$SOAKMINP3P1
  vess_id[c((ind1[i-2016]+sub_n_frame$fix_n[i-2016]):ind2[i-2016])]<-sub_strat_dat_comb[[i-2016]]$Vess_fact
  locations[c((ind1[i-2016]+sub_n_frame$fix_n[i-2016]):ind2[i-2016]),]<-st_coordinates(sub_strat_dat_comb[[i-2016]])
}
H_i<-which(H_j==1)-1
#Change vessel id so that they're ordered, kindoff
garb<-unique(vess_id)
garb2<-order(unique(vess_id))
for (j in 1:length(vess_id)){
  for (i in 1:length(garb)){
    if (vess_id[j]==garb[i]){
      vess_id[j]<-garb2[i]
      break
    }
  }
}
vess_id<-as.integer(as.factor(vess_id))
vess_id<-vess_id-1

spool<-sqrt(((length(locations[,1])-1)*var(locations[,1])+(length(locations[,2])-1)*var(locations[,2]))/(length(locations[,1])+length(locations[,2])))
locations[,1]<-(locations[,1]-median(locations[,1]))/spool
locations[,2]<-(locations[,2]-median(locations[,2]))/spool


big_H<-as.matrix(data.frame(c(data2017$empty_baited,data2018$empty_baited,data2019$empty_baited,data2020$empty_baited,data2021$empty_baited),c(data2017$target_species,data2018$target_species,data2019$target_species,data2020$target_species,data2021$target_species),c(data2017$other_species,data2018$other_species,data2019$other_species,data2020$other_species,data2021$other_species),c(data2017$empty_unb,data2018$empty_unb,data2019$empty_unb,data2020$empty_unb,data2021$empty_unb)))

data<-list(A=big_A,H=big_H,H_i=H_i,H_j=H_j,vess_id=vess_id,s=soaky,locations=locations,n_t=n_t,n=main_n,n_k=3,n2=main_n2,n_k2=4)

par_list<-list()
    
# Initial values for lambda.t, lambda.nt and pnt
# Use the estimated values as the starting points
par_list$betat = -12
par_list$betant = -8
# par_list$theta = logitp(0.8795768)
par_list$theta = 0
#Random effect of time
par_list$rand_t = rep(-12,data$n_t)
par_list$rand_nt = rep(-8,data$n_t)
# Random field
par_list$omegat =  rep(0,nrow(big_A))
par_list$omegant =  rep(0,nrow(big_A))
#Vessel effect
par_list$vess_eff_t = rep(0,length(unique(vess_id)))
par_list$vess_eff_nt = rep(0,length(unique(vess_id)))
# Smoothness parameter 
par_list$lognut = 0
par_list$lognunt = 0
# Range parameter 
# par_list$logPhit = -3
# par_list$logPhint = -1
par_list$logPhit = -1
par_list$logPhint = -1
# Variance
par_list$logSigmat = 0
par_list$logSigmant = 0
#Vess effect sigmas
par_list$log_sigma_vess_t = 0
par_list$log_sigma_vess_nt = 0
#Rand intercept sigmas
par_list$log_sigma_rand_t = 0
par_list$log_sigma_rand_nt = 0
#Rand intercept means
par_list$mean_rand_t = -12
par_list$mean_rand_nt = -8
par_list$mean_ar_t = -12
par_list$mean_ar_nt = -8
par_list$logit_phi_ar_t<-0
par_list$logit_phi_ar_nt<-0

par_list$log_H_aniso = c(0,0)

map<-list(lognut=factor(NA),lognunt=factor(NA))

random<-c("omegat","omegant","rand_t","rand_nt","vess_eff_t","vess_eff_nt")
library(optimr)
```

```{r fit-2017}



compile("spat_temp_both_datasets.cpp")
dyn.load(dynlib("spat_temp_both_datasets"))

obj_walk<-MakeADFun(data,par_list,random=random,DLL="spat_temp_both_datasets",map=map,silent=F)
Opt_walk<-optimx::optimr(obj_walk$par,obj_walk$fn,obj_walk$gr,control=list(maxit=100000),method="nlminb")
if (Opt_walk$message=="iteration limit reached without convergence (10)") {
      obj_walk$par<-obj_walk$env$last.par[which(names(obj_walk$env$last.par) %in% non_r)]
      Opt_walk <- try(optimx::optimr(obj_walk$par,obj_walk$fn,obj_walk$gr,control=control,method="nlminb"),T)
    }
rep_walk<-sdreport(obj_walk)
Report_walk<-obj_walk$report()

save(obj_walk,Opt_walk,rep_walk,Report_walk,file="rand_walk_output_2017.RData")

dyn.unload(dynlib("spat_temp_both_datasets"))
compile("spat_temp_mean.cpp")
dyn.load(dynlib("spat_temp_mean"))

obj_mean<-MakeADFun(data,par_list,random=random,DLL="spat_temp_mean",map=map,silent=F)
Opt_mean<-optimx::optimr(obj_mean$par,obj_mean$fn,obj_mean$gr,control=list(maxit=100000),method="nlminb")
if (Opt_mean$message=="iteration limit reached without convergence (10)") {
      obj_mean$par<-obj_mean$env$last.par[which(names(obj_mean$env$last.par) %in% non_r)]
      Opt_mean <- try(optimx::optimr(obj_mean$par,obj_mean$fn,obj_mean$gr,control=control,method="nlminb"),T)
    }
rep_mean<-sdreport(obj_mean)
Report_mean<-obj_mean$report()

save(obj_mean,Opt_mean,rep_mean,Report_mean,file="rand_mean_output_2017.RData")

dyn.unload(dynlib("spat_temp_mean"))
compile("spat_temp_slope.cpp")
dyn.load(dynlib("spat_temp_slope"))

par_list$rand_t = rep(0,data$n_t)
par_list$rand_nt = rep(0,data$n_t)

obj_slope<-MakeADFun(data,par_list,random=random,DLL="spat_temp_slope",map=map,silent=F)
Opt_slope<-optimx::optimr(obj_slope$par,obj_slope$fn,obj_slope$gr,control=list(maxit=100000),method="nlminb")
if (Opt_slope$message=="iteration limit reached without convergence (10)") {
      obj_slope$par<-obj_slope$env$last.par[which(names(obj_slope$env$last.par) %in% non_r)]
      Opt_slope <- try(optimx::optimr(obj_slope$par,obj_slope$fn,obj_slope$gr,control=control,method="nlminb"),T)
    }
rep_slope<-sdreport(obj_slope)
Report_slope<-obj_slope$report()

save(obj_slope,Opt_slope,rep_slope,Report_slope,file="rand_slope_output_2017.RData")

dyn.unload(dynlib("spat_temp_slope"))
compile("spat_temp_AR1.cpp")
dyn.load(dynlib("spat_temp_AR1"))

par_list$rand_t = rep(-12,data$n_t)
par_list$rand_nt = rep(-8,data$n_t)

obj_AR1<-MakeADFun(data,par_list,random=random,DLL="spat_temp_AR1",map=map,silent=F)
Opt_AR1<-optimx::optimr(obj_AR1$par,obj_AR1$fn,obj_AR1$gr,control=list(maxit=100000),method="nlminb")
if (Opt_AR1$message=="iteration limit reached without convergence (10)") {
      obj_AR1$par<-obj_AR1$env$last.par[which(names(obj_AR1$env$last.par) %in% non_r)]
      Opt_AR1 <- try(optimx::optimr(obj_AR1$par,obj_AR1$fn,obj_AR1$gr,control=control,method="nlminb"),T)
    }
rep_AR1<-sdreport(obj_AR1)
Report_AR1<-obj_AR1$report()

save(obj_AR1,Opt_AR1,rep_AR1,Report_AR1,file="rand_AR1_output_2017.RData")

dyn.unload(dynlib("spat_temp_AR1"))


```

```{r prep-strat-only-2017}
n_strat<-sum(c(nrow(data2017),nrow(data2018),nrow(data2019),nrow(data2020),nrow(data2021)))
n_strats_sep<-c(nrow(data2017),nrow(data2018),nrow(data2019),nrow(data2020),nrow(data2021))
n_fixed<-0

#Setting up H_i and H_j
H_j<-rep(NA,n_strat+n_fixed)
H_i<-rep(NA,n_strat)
vess_id<-rep(NA,n_strat+n_fixed)

n_t<-length(2017:2021)
n_k<-3
n_k2<-4

main_n2<-rep(NA,n_t)
for (i in 1:n_t){
  main_n2[i]<-n_strats_sep[i]
}

main_n<-rep(NA,n_t)
for (i in 1:n_t){
  main_n[i]<-n_frame$fix_n[i+19]+main_n2[i]
}

sub_n_frame<-n_frame[c(20:24),]

#NEED TO MODIFY STARTING HERE
Yearly_indices<-data.frame(year=c(2017:2021),
                           ind_1=c(1,142,292,415,563),
                           ind_2=c(141,291,414,562,693))
ind1<-Yearly_indices$ind_1
ind2<-Yearly_indices$ind_2

soaky<-rep(NA,sum(main_n2))

locations<-matrix(nrow=sum(main_n2),ncol=2)

sub_strat_dat_comb<-list(strat_dat_comb[[18]],strat_dat_comb[[19]],strat_dat_comb[[20]],strat_dat_comb[[21]],strat_dat_comb[[22]])

big_A<-matrix(nrow=sum(main_n2),ncol=n_k)
for (i in 2017:2021){
    big_A[c((ind1[i-2016]):ind2[i-2016]),]<-as.matrix(data.frame(sub_strat_dat_comb[[i-2016]]$target_species_700,sub_strat_dat_comb[[i-2016]]$other_species_700,sub_strat_dat_comb[[i-2016]]$nbe_700))
  H_j[c((ind1[i-2016]):ind2[i-2016])]<-rep(1,length(c((ind1[i-2016]):ind2[i-2016])))
  soaky[c((ind1[i-2016]):ind2[i-2016])]<-sub_strat_dat_comb[[i-2016]]$SOAKMINP3P1
  vess_id[c((ind1[i-2016]):ind2[i-2016])]<-sub_strat_dat_comb[[i-2016]]$Vess_fact
  locations[c((ind1[i-2016]):ind2[i-2016]),]<-st_coordinates(sub_strat_dat_comb[[i-2016]])
}
H_i<-which(H_j==1)-1
garb<-unique(vess_id)
garb2<-order(unique(vess_id))
for (j in 1:length(vess_id)){
  for (i in 1:length(garb)){
    if (vess_id[j]==garb[i]){
      vess_id[j]<-garb2[i]
      break
    }
  }
}
vess_id<-as.integer(as.factor(vess_id))
vess_id<-vess_id-1


spool<-sqrt(((length(locations[,1])-1)*var(locations[,1])+(length(locations[,2])-1)*var(locations[,2]))/(length(locations[,1])+length(locations[,2])))
locations[,1]<-(locations[,1]-median(locations[,1]))/spool
locations[,2]<-(locations[,2]-median(locations[,2]))/spool


big_H<-as.matrix(data.frame(c(data2017$empty_baited,data2018$empty_baited,data2019$empty_baited,data2020$empty_baited,data2021$empty_baited),c(data2017$target_species,data2018$target_species,data2019$target_species,data2020$target_species,data2021$target_species),c(data2017$other_species,data2018$other_species,data2019$other_species,data2020$other_species,data2021$other_species),c(data2017$empty_unb,data2018$empty_unb,data2019$empty_unb,data2020$empty_unb,data2021$empty_unb)))

data<-list(A=big_A,H=big_H,H_i=H_i,H_j=H_j,s=soaky,vess_id=vess_id,locations=locations,n_t=n_t,n=main_n2,n_k=3,n2=main_n2,n_k2=4)

par_list<-list()
    
# Initial values for lambda.t, lambda.nt and pnt
# Use the estimated values as the starting points
par_list$betat = -13
par_list$betant = -8
# par_list$theta = logitp(0.8795768)
par_list$theta = 0
#Random effect of time
par_list$rand_t = rep(-13,data$n_t)
par_list$rand_nt = rep(-8,data$n_t)
# Random field
par_list$omegat =  rep(0,nrow(big_A))
par_list$omegant =  rep(0,nrow(big_A))
#Vessel effect
par_list$vess_eff_t = rep(0,length(unique(vess_id)))
par_list$vess_eff_nt = rep(0,length(unique(vess_id)))
# Smoothness parameter 
par_list$lognut = 0
par_list$lognunt = 0
# Range parameter 
# par_list$logPhit = -3
# par_list$logPhint = -1
par_list$logPhit = -1
par_list$logPhint = -1
# Variance
par_list$logSigmat = 0
par_list$logSigmant = 0
#Vess effect sigmas
par_list$log_sigma_vess_t = 0
par_list$log_sigma_vess_nt = 0
#Rand intercept sigmas
par_list$log_sigma_rand_t = 0
par_list$log_sigma_rand_nt = 0
#Rand intercept means
par_list$mean_rand_t = -12
par_list$mean_rand_nt = -8
par_list$mean_ar_t = -12
par_list$mean_ar_nt = -8
par_list$logit_phi_ar_t<-0
par_list$logit_phi_ar_nt<-0


par_list$log_H_aniso = c(0,0)

map<-list(lognut=factor(NA),lognunt=factor(NA))

random<-c("omegat","omegant","rand_t","rand_nt","vess_eff_t","vess_eff_nt")
library(optimr)

```

```{r fit-strat-2017}

compile("spat_temp_both_datasets.cpp")
dyn.load(dynlib("spat_temp_both_datasets"))

obj_walk<-MakeADFun(data,par_list,random=random,DLL="spat_temp_both_datasets",map=map,silent=F)
Opt_walk<-optimx::optimr(obj_walk$par,obj_walk$fn,obj_walk$gr,control=list(maxit=100000),method="nlminb")
if (Opt_walk$message=="iteration limit reached without convergence (10)") {
      obj_walk$par<-obj_walk$env$last.par[which(names(obj_walk$env$last.par) %in% non_r)]
      Opt_walk <- try(optimx::optimr(obj_walk$par,obj_walk$fn,obj_walk$gr,control=control,method="nlminb"),T)
    }
rep_walk<-sdreport(obj_walk)
Report_walk<-obj_walk$report()

save(obj_walk,Opt_walk,rep_walk,Report_walk,file="rand_walk_output_strat_2017.RData")

dyn.unload(dynlib("spat_temp_both_datasets"))
compile("spat_temp_mean.cpp")
dyn.load(dynlib("spat_temp_mean"))

obj_mean<-MakeADFun(data,par_list,random=random,DLL="spat_temp_mean",map=map,silent=F)
Opt_mean<-optimx::optimr(obj_mean$par,obj_mean$fn,obj_mean$gr,control=list(maxit=100000),method="nlminb")
if (Opt_mean$message=="iteration limit reached without convergence (10)") {
      obj_mean$par<-obj_mean$env$last.par[which(names(obj_mean$env$last.par) %in% non_r)]
      Opt_mean <- try(optimx::optimr(obj_mean$par,obj_mean$fn,obj_mean$gr,control=control,method="nlminb"),T)
    }
rep_mean<-sdreport(obj_mean)
Report_mean<-obj_mean$report()

save(obj_mean,Opt_mean,rep_mean,Report_mean,file="rand_mean_output_strat_2017.RData")

dyn.unload(dynlib("spat_temp_mean"))
compile("spat_temp_slope.cpp")
dyn.load(dynlib("spat_temp_slope"))

par_list$rand_t = rep(0,data$n_t)
par_list$rand_nt = rep(0,data$n_t)

obj_slope<-MakeADFun(data,par_list,random=random,DLL="spat_temp_slope",map=map,silent=F)
Opt_slope<-optimx::optimr(obj_slope$par,obj_slope$fn,obj_slope$gr,control=list(maxit=100000),method="nlminb")
if (Opt_slope$message=="iteration limit reached without convergence (10)") {
      obj_slope$par<-obj_slope$env$last.par[which(names(obj_slope$env$last.par) %in% non_r)]
      Opt_slope <- try(optimx::optimr(obj_slope$par,obj_slope$fn,obj_slope$gr,control=control,method="nlminb"),T)
    }
rep_slope<-sdreport(obj_slope)
Report_slope<-obj_slope$report()

save(obj_slope,Opt_slope,rep_slope,Report_slope,file="rand_slope_output_strat_2017.RData")

dyn.unload(dynlib("spat_temp_slope"))
compile("spat_temp_AR1.cpp")
dyn.load(dynlib("spat_temp_AR1"))

par_list$rand_t = rep(-12,data$n_t)
par_list$rand_nt = rep(-8,data$n_t)

obj_AR1<-MakeADFun(data,par_list,random=random,DLL="spat_temp_AR1",map=map,silent=F)
Opt_AR1<-optimx::optimr(obj_AR1$par,obj_AR1$fn,obj_AR1$gr,control=list(maxit=100000),method="nlminb")
if (Opt_AR1$message=="iteration limit reached without convergence (10)") {
      obj_AR1$par<-obj_AR1$env$last.par[which(names(obj_AR1$env$last.par) %in% non_r)]
      Opt_AR1 <- try(optimx::optimr(obj_AR1$par,obj_AR1$fn,obj_AR1$gr,control=control,method="nlminb"),T)
    }
rep_AR1<-sdreport(obj_AR1)
Report_AR1<-obj_AR1$report()

save(obj_AR1,Opt_AR1,rep_AR1,Report_AR1,file="rand_AR1_output_strat_2017.RData")

dyn.unload(dynlib("spat_temp_AR1"))

```

```{r}
#This is mostly about obtaining spatially-weighed average, with just the full fit with random walk as example

bid = read.csv("blockIDkey.csv", header = T)
loc_id = cbind(bid$lon.DecDeg, bid$lat.DecDeg)
# Plot Block ID
plot(loc_id, xlab="Long", ylab="Lat", main="blockIDkey", pch=20)

load("good_bound.RData")
make_20<-st_as_sf(tf_bound_a_pos22,coords=c("long","lat"))
st_crs(make_20)<-CRS("+init=epsg:32619")
make_20<-st_transform(make_20,utm.prj4s)

library(gissr)
library(deldir)
library(spatstat)
library(alphahull)

pp_list<-list()
diri_list<-list()
for (i in c(2000:2021)){
  temp<-subset(sf_rem_fix_data,YEAR==i)
  temp_dist<-st_coordinates(temp)
  
  pp_list[[i-1999]]=ppp(temp_dist[,1], temp_dist[,2], window=owin(poly=list(x=st_coordinates(make_20)[,1],y=st_coordinates(make_20)[,2])))
          
  diri_list[[i-1999]] = dirichlet(pp_list[[i-1999]])
}

library(raster)
#What if I do the dumb way?
sf_loc_id<-st_as_sf(as.data.frame(loc_id),coords=c("V1","V2"),crs=prj4s) %>% 
  st_transform(utm.prj4s)
station_IDs<-list()
for (i in c(1:length(diri_list))){
  temp<-subset(sf_rem_fix_data,YEAR==(i+1999))
  temp_dist<-st_coordinates(temp)
  
  temp_distance<-pointDistance(sf_loc_id,temp_dist,lonlat=F)
  
  station_ID<-c()
  
  for (j in 1:length(st_coordinates(sf_loc_id)[,1])) {
  station_ID<-c(station_ID,which(temp_distance[j,]==min(temp_distance[j,])))
  }
  
   station_IDs[[i]]<-station_ID
  
}

load("rand_walk_output.RData")

# Area
area_list<-list()
for (i in c(1:length(diri_list))){
  wts = c()
  for (j in 1:length(Report$ldat[Yearly_indices[i,]$ind_1:Yearly_indices[i,]$ind_2])){
    temp_area<-length(which(station_IDs[[i]]==j))*4
    wts[j]<-temp_area
  }
  # Target
  dweight_ave_t = sum((Report$ldat[Yearly_indices[i,]$ind_1:Yearly_indices[i,]$ind_2])*wts)/sum(wts)
  weighted_t<-dweight_ave_t
  # Standard error calculation
  wts = as.matrix(wts)
  # wi/sum(wi)
  wts_2 = wts/(sum(wts))
  # covariance matrix for estimated lambda t
  cov = rep$cov
  cov_t = cov[(Yearly_indices[i,]$ind_1+49):(Yearly_indices[i,]$ind_2+49), (Yearly_indices[i,]$ind_1+49):(Yearly_indices[i,]$ind_2+49)]
  
  # Standard error for Dirichlet method 
  se2_dir_t = t(wts_2)%*%cov_t%*%wts_2
  se_t<-se2_dir_t
  # Non-target
  dweight_ave_nt = sum((Report$ldant[Yearly_indices[i,]$ind_1:Yearly_indices[i,]$ind_2])*wts)/sum(wts)
  weighted_nt<-dweight_ave_nt
  # covariance matrix for estimated lambda t
  cov_nt = cov[(tail(Yearly_indices$ind_2,n=1)+Yearly_indices[i,]$ind_1+49):(tail(Yearly_indices$ind_2,n=1)+Yearly_indices[i,]$ind_2+49), (tail(Yearly_indices$ind_2,n=1)+Yearly_indices[i,]$ind_1+49) :(tail(Yearly_indices$ind_2,n=1)+Yearly_indices[i,]$ind_2+49)]
  # Standard error for Dirichlet method 
  se2_dir_nt = t(wts_2)%*%cov_nt%*%wts_2
  se_nt<-se2_dir_nt
  
  area_list[[i]]<-data.frame(weighted_t,sqrt(se_t),weighted_nt,sqrt(se_nt))
}

weighted_avgs_frame<-data.frame(w_t=rep(NA,22),se_t=rep(NA,22),
                                w_nt=rep(NA,22),se_nt=rep(NA,22))
for (i in c(1:22)){
  weighted_avgs_frame$w_t[i]<-area_list[[i]]$weighted_t
  weighted_avgs_frame$se_t[i]<-area_list[[i]]$sqrt.se_t.
  weighted_avgs_frame$w_nt[i]<-area_list[[i]]$weighted_nt
  weighted_avgs_frame$se_nt[i]<-area_list[[i]]$sqrt.se_nt.
}

w_lamb_t<-ggplot(data=weighted_avgs_frame)+
  geom_point(aes(x=2000:2021,y=w_t),col="red")+
  geom_line(aes(x=2000:2021,y=w_t),col="red")+
  geom_ribbon(aes(x=2000:2021,ymax=w_t+2*se_t,ymin=w_t-2*se_t),fill="red",alpha=0.2)+
  theme_bw()

w_lamb_nt<-ggplot(data=weighted_avgs_frame)+
  geom_point(aes(x=2000:2021,y=w_nt),col="blue")+
  geom_line(aes(x=2000:2021,y=w_nt),col="blue")+
  geom_ribbon(aes(x=2000:2021,ymax=w_nt+2*se_nt,ymin=w_nt-2*se_nt),fill="blue",alpha=0.2)+
  theme_bw()



```

```{r}
#Obtaining stratified mean estimates to compare to
temp_rem_fix_data<-sf_rem_fix_data[,-16]
temp_sf_all_dat<-rbind(subset(temp_rem_fix_data,YEAR<2018),
                       data2017[,which(colnames(data2017) %in% colnames(temp_rem_fix_data))],subset(temp_rem_fix_data,YEAR==2018),data2018[,which(colnames(data2018) %in% colnames(temp_rem_fix_data))],subset(temp_rem_fix_data,YEAR==2019),data2019[,which(colnames(data2019) %in% colnames(temp_rem_fix_data))],subset(temp_rem_fix_data,YEAR==2020),data2020[,which(colnames(data2020) %in% colnames(temp_rem_fix_data))],subset(temp_rem_fix_data,YEAR==2021),data2021[,which(colnames(data2021) %in% colnames(temp_rem_fix_data))])

#Taking averages
agg_mean<-aggregate(total_target_species/(SOAKMINP3P1*NUM_HOOK_HAUL)~YEAR,data=temp_sf_all_dat,FUN=mean)
agg_sd<-aggregate(total_target_species/(SOAKMINP3P1*NUM_HOOK_HAUL)~YEAR,data=temp_sf_all_dat,FUN=sd)
agg_n<-aggregate(total_target_species/(SOAKMINP3P1*NUM_HOOK_HAUL)~YEAR,data=temp_sf_all_dat,FUN=length)

agg_mean_fixed<-aggregate(total_target_species/(SOAKMINP3P1*NUM_HOOK_HAUL)~YEAR,data=sf_rem_fix_data,FUN=mean)
agg_sd_fixed<-aggregate(total_target_species/(SOAKMINP3P1*NUM_HOOK_HAUL)~YEAR,data=sf_rem_fix_data,FUN=sd)
agg_n_fixed<-aggregate(total_target_species/(SOAKMINP3P1*NUM_HOOK_HAUL)~YEAR,data=sf_rem_fix_data,FUN=length)

agg_sd$`total_target_species/(SOAKMINP3P1 * NUM_HOOK_HAUL)`<-agg_sd$`total_target_species/(SOAKMINP3P1 * NUM_HOOK_HAUL)`/sqrt(agg_n$`total_target_species/(SOAKMINP3P1 * NUM_HOOK_HAUL)`)
agg_sd_fixed$`total_target_species/(SOAKMINP3P1 * NUM_HOOK_HAUL)`<-agg_sd_fixed$`total_target_species/(SOAKMINP3P1 * NUM_HOOK_HAUL)`/sqrt(agg_n_fixed$`total_target_species/(SOAKMINP3P1 * NUM_HOOK_HAUL)`)

colnames(agg_mean)<-c("Year","Index")
colnames(agg_sd)<-c("Year","se")
colnames(agg_mean_fixed)<-c("Year","Index")
colnames(agg_sd_fixed)<-c("Year","se")

avg_weights[is.na(avg_weights)]<-0
```
